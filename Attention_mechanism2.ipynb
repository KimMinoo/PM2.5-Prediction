{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimMinoo/PM2.5-Prediction/blob/main/Attention_mechanism2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5gGk7nTUkXP_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as datasets\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kjid0KAGLrWF",
        "outputId": "97899829-0e3f-4aee-d3b3-49946757bf8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihHOSfR6YFCi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# 데이터 불러오기\n",
        "# test_aws = pd.read_csv('/content/test_aws')\n",
        "# test_pm = pd.read_csv('/content/test_pm')\n",
        "# train_aws = pd.read_csv('/content/train_aws')\n",
        "# train_pm = pd.read_csv('/content/train_pm')\n",
        "\n",
        "# train_df = pd.merge(test_aws, test_pm)\n",
        "# test_df = pd.merge(train_aws, train_pm)\n",
        "train_df = pd.read_csv('/content/drive/Shareddrives/미세먼지_예측_대회/TRAIN_ALL/TRAIN_4_real_final.csv')\n",
        "# test_df = pd.read_csv('/content/공주_val.csv')\n",
        "\n",
        "del train_df['일시']\n",
        "# del test_df['일시']\n",
        "# del train_df['강수량(mm)']\n",
        "# del test_df['강수량(mm)']\n",
        "\n",
        "# del train_df['풍향(deg)']\n",
        "# del test_df['풍향(deg)']\n",
        "# del train_df['풍속(m/s)']\n",
        "# del test_df['풍속(m/s)']\n",
        "\n",
        "train_df=train_df.interpolate()\n",
        "# test_df=test_df.interpolate()\n",
        "\n",
        "train_df = train_df.dropna()\n",
        "# test_df = train_df.dropna()\n",
        "\n",
        "# train_size = int(len(train_df)*0.9)\n",
        "# train_set = train_df[0:train_size]  \n",
        "# val_set = train_df[train_size:]\n",
        "# test_set = test_df\n",
        "\n",
        "train_size = 35063*27\n",
        "train_set = train_df[0:train_size]\n",
        "val_set = train_df[train_size:]\n",
        "\n",
        "\n",
        "# train_set = train_set.transpose()\n",
        "# val_set = val_set.transpose()\n",
        "# test_set = test_set.transpose()\n",
        "\n",
        "# 7일간의 데이터가 입력으로 들어가고 batch size는 임의로 지정\n",
        "seq_length = 48\n",
        "batch = 128\n",
        "\n",
        "# 데이터를 역순으로 정렬하여 전체 데이터의 70% 학습, 30% 테스트에 사용\n",
        "\n",
        "\n",
        "# train_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vlfAXlZFYQ4w"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset # 텐서데이터셋\n",
        "from torch.utils.data import DataLoader # 데이터로더\n",
        "\n",
        "k=[]\n",
        "for j in range(30):\n",
        "    for i in range(seq_length):\n",
        "      k.append(35063*(j+1)-seq_length+i+1)\n",
        "\n",
        "# 데이터셋 생성 함수\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series)-seq_length-72):\n",
        "        _x = time_series[i:i+seq_length, :].transpose()\n",
        "        _y = time_series[i+seq_length:i+seq_length+72, -1].reshape(-1,1)\n",
        "        if i in k:\n",
        "          continue\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# # 데이터셋 생성 함수\n",
        "# def AWS_build_dataset(time_series, seq_length):\n",
        "#     dataX = []\n",
        "#     dataY = []\n",
        "#     for i in range(0, len(time_series)-seq_length):\n",
        "#         _x = time_series[i:i+seq_length, :-1]\n",
        "#         _y = time_series[i+seq_length, :-1]\n",
        "#         dataX.append(_x)\n",
        "#         dataY.append(_y)\n",
        "#     return np.array(dataX), np.array(dataY)\n",
        "\n",
        "trainX, trainY = build_dataset(np.array(train_set), seq_length)\n",
        "valX, valY = build_dataset(np.array(val_set), seq_length)\n",
        "# testX, testY = build_dataset(np.array(test_set), seq_length)\n",
        "\n",
        "trainX_tensor = torch.FloatTensor(trainX).cuda()\n",
        "trainY_tensor = torch.FloatTensor(trainY).cuda()\n",
        "\n",
        "valX_tensor = torch.FloatTensor(valX).cuda()\n",
        "valY_tensor = torch.FloatTensor(valY).cuda()\n",
        "\n",
        "# testX_tensor = torch.FloatTensor(testX).cuda()\n",
        "# testY_tensor = torch.FloatTensor(testY).cuda()\n",
        "\n",
        "train_dataset = TensorDataset(trainX_tensor, trainY_tensor)\n",
        "val_dataset = TensorDataset(valX_tensor, valY_tensor)\n",
        "# test_dataset = TensorDataset(testX_tensor, testY_tensor)\n",
        "\n",
        "# 데이터로더는 기본적으로 2개의 인자를 입력받으며 배치크기는 통상적으로 2의 배수를 사용\n",
        "train_dataloader = DataLoader(train_dataset,batch_size=batch, shuffle=True, drop_last=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
        "# test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=False, drop_last=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = iter(train_dataloader)\n",
        "data1, data2 = next(test)\n",
        "rnn = nn.GRU(48, 72, batch_first=True, bidirectional=True).cuda()\n",
        "outputs, hidden = rnn(data1)\n",
        "print(hidden.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSm2q0jguYI1",
        "outputId": "81eb0312-fa7d-4057-ca3a-34a2dd315428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 128, 72])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XR_s8rrXYQ7D"
      },
      "outputs": [],
      "source": [
        "# 설정값\n",
        "input_dim = 48\n",
        "encoder_hidden_dim = 36\n",
        "decoder_hidden_dim = 18\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "nb_epochs = 1500\n",
        "\n",
        "class encoder(nn.Module):\n",
        "    # # 기본변수, layer를 초기화해주는 생성자\n",
        "    def __init__(self, input_dim, encoder_hidden_dim, decoder_hidden_dim):\n",
        "        super(encoder, self).__init__()\n",
        "        self.rnn = nn.GRU(input_dim, encoder_hidden_dim,dropout = 0.15, batch_first=True, bidirectional =True)\n",
        "        self.fc = nn.Linear(encoder_hidden_dim*2, decoder_hidden_dim)\n",
        "\n",
        "    # 예측을 위한 함수\n",
        "    def forward(self, x):\n",
        "        outputs, hidden = self.rnn(x)\n",
        "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim = 1)))\n",
        "\n",
        "        return outputs, hidden.unsqueeze(0)\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_hidden_dim, decoder_hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attention = nn.Linear(encoder_hidden_dim*2 + decoder_hidden_dim, decoder_hidden_dim)\n",
        "        self.v = nn.Linear(decoder_hidden_dim, 1, bias = False)\n",
        "        \n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        \n",
        "        # hidden = [배치 사이즈, 디코더 히든 차원]\n",
        "        # encoder_outputs = [단어 개수, 배치 사이즈, 인코더 히든 차원*2]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[0] # 배치 사이즈\n",
        "        src_len = encoder_outputs.shape[1] # 단어 개수\n",
        "        \n",
        "        hidden = hidden.reshape(batch_size, 1, -1)\n",
        "        hidden = hidden.repeat(1, src_len, 1)\n",
        "\n",
        "        # hidden = [배치 사이즈, 단어 개수, 디코더 히든 차원]\n",
        "        # encoder_outputs = [ 배치 사이즈, 단어 개수,인코더 히든 차원]\n",
        "        \n",
        "        \n",
        "        # 바다나우 어텐션        \n",
        "        energy = torch.tanh(self.attention(torch.cat([hidden, encoder_outputs], dim = 2)))\n",
        "\n",
        "        # softmax 정규화      \n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        \n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class decoder(nn.Module):\n",
        "    # # 기본변수, layer를 초기화해주는 생성자\n",
        "    def __init__(self, decoder_input_dim, decoder_output_dim, encoder_hidden_dim, decoder_hidden_dim, attention):\n",
        "        super(decoder, self).__init__()\n",
        "        self.decoder_output_dim = decoder_output_dim\n",
        "        self.attention = attention\n",
        "\n",
        "        self.rnn = nn.GRU(decoder_input_dim+ encoder_hidden_dim*2, decoder_hidden_dim, batch_first=True)\n",
        "        self.fc_out = nn.Linear(encoder_hidden_dim*2 + decoder_hidden_dim + decoder_input_dim, decoder_output_dim)\n",
        "\n",
        "    # 예측을 위한 함수\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        \n",
        "        a = self.attention(hidden, encoder_outputs)\n",
        "        a = a.unsqueeze(1)\n",
        "        weighted = torch.bmm(a, encoder_outputs)\n",
        "\n",
        "        \n",
        "        rnn_input = torch.cat([input, weighted], dim = 2)\n",
        "\n",
        "        output, hidden = self.rnn(rnn_input, hidden)\n",
        "\n",
        "        assert (output == hidden.permute(1,0,2)).all()\n",
        "        \n",
        "        input = input.squeeze(1)\n",
        "        output = output.squeeze(1)\n",
        "        weighted = weighted.squeeze(1)\n",
        "        \n",
        "        prediction = self.fc_out(torch.cat([output, weighted, input], dim = 1))\n",
        "        prediction = prediction.reshape(encoder_outputs.shape[0], 1, self.decoder_output_dim)\n",
        "        return prediction, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aILNx19Wiz_r"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\n",
        "\n",
        "    def forward(self, encoder_input, decoder_input):\n",
        "        # src: [src len, batch size]\n",
        "        # trg: [trg len, batch size]\n",
        "        \n",
        "        batch_size = encoder_input.shape[0]\n",
        "        dec_len = 72\n",
        "\n",
        "        outputs = torch.zeros(encoder_input.shape[0], encoder_input.shape[1], 72).to(self.device)\n",
        "\n",
        "        encoder_outputs, hidden = self.encoder(encoder_input)\n",
        "\n",
        "        outputs = torch.empty(batch_size, 0, 1).cuda()\n",
        "        # input = encoder_input[:,-1,[-1]].unsqueeze(1).to(self.device)\n",
        "        \n",
        "        input = torch.zeros(batch_size, 1, 1).to(self.device)\n",
        "        for i in range(0,dec_len): # <eos> 제외하고 trg_len-1 만큼 반복\n",
        "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
        "            \n",
        "            outputs = torch.cat([outputs,output],dim=1)\n",
        "\n",
        "            # teacher forcing을 사용할지, 말지 결정\n",
        "            if self.teacher_forcing_ratio != 0:\n",
        "                teacher_force = random.random() < self.teacher_forcing_ratio\n",
        "                input = decoder_input[:,i,[-1]].unsqueeze(1) if teacher_force else output\n",
        "            elif self.teacher_forcing_ratio == 0:\n",
        "                input = output\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihkyPXfpYQ9l"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train():\n",
        "    start_time = time.time()\n",
        "    model.train()\n",
        "\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        x, y = batch\n",
        "            \n",
        "        output = model(x,y)\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(output, y)\n",
        "        loss = torch.sqrt(loss)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "            \n",
        "        total += 1\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss/total\n",
        "\n",
        "def validate():\n",
        "    start_time = time.time()\n",
        "    print(f'[Epoch: {epoch + 1} - Training]')\n",
        "    model.eval()\n",
        "\n",
        "    total = 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, batch in enumerate(val_dataloader):\n",
        "        x, y = batch\n",
        "\n",
        "        with torch.no_grad(): \n",
        "          output = model(x,y)\n",
        "          loss = criterion(output, y)\n",
        "          loss = torch.sqrt(loss)\n",
        "          \n",
        "        total += 1\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    return running_loss/total\n",
        "\n",
        "# def test():\n",
        "#     start_time = time.time()\n",
        "#     print(f'[Epoch: {epoch + 1} - Training]')\n",
        "#     model.eval()\n",
        "\n",
        "#     total = 0\n",
        "#     running_loss = 0.0\n",
        "\n",
        "#     for i, batch in enumerate(test_dataloader):\n",
        "#         x, y = batch\n",
        "\n",
        "#         with torch.no_grad(): \n",
        "#           output = model(x,y)\n",
        "#           loss = criterion(output, y)\n",
        "            \n",
        "#         total += 1\n",
        "#         running_loss += loss.item()\n",
        "\n",
        "#     return running_loss/total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwG0a2HF6xna"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# def adjust_learning_rate(optimizer, epoch):\n",
        "#   lr = learning_rate\n",
        "#   if epoch % 10 == 0:\n",
        "#     lr /= 5\n",
        "#   for param_group in optimizer.param_groups:\n",
        "#     param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7bAwG-WYRBy",
        "outputId": "ebd9a30e-5785-48fc-879c-fe4c56f2b41e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch: 1 - Training]\n",
            "train loss: 0.0013831890300592532 \tval_loss: 0.0009925746258309894\n",
            "[Info] best validation accuracy!\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.0001\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "enc = encoder(input_dim, encoder_hidden_dim, decoder_hidden_dim).cuda()\n",
        "attention = Attention(encoder_hidden_dim, decoder_hidden_dim).cuda()\n",
        "dec = decoder(1, 1,  encoder_hidden_dim, decoder_hidden_dim, attention).cuda()\n",
        "model = Seq2Seq(enc, dec, device, 0.5).to(device)\n",
        "# model.load_state_dict(torch.load(\"/content/drive/Shareddrives/미세먼지_예측_대회/김민우/pretrained2.pth\"))\n",
        "model = model.cuda()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_epochs = 11\n",
        "best_epoch = 0\n",
        "verbose = 1\n",
        "patience = 11\n",
        "best_val_loss = 5\n",
        "\n",
        "history = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # adjust_learning_rate(optimizer, epoch)\n",
        "    train_loss = train()\n",
        "    val_loss = validate()\n",
        "    history.append((train_loss, val_loss))\n",
        "\n",
        "    if epoch % verbose == 0:\n",
        "        print('train loss:',train_loss,'\\tval_loss:',val_loss)\n",
        "            \n",
        "    #     # patience번째 마다 early stopping 여부 확인\n",
        "    # if (epoch % patience == 0) & (epoch != 0):\n",
        "    #     if history[epoch][0] < history[epoch-10][0]:\n",
        "    #             print('\\n Early Stopping')\n",
        "    #             break\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        print(\"[Info] best validation accuracy!\")\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch = epoch\n",
        "        torch.save(model.state_dict(), \"best_checkpoint_epoch.pth\")\n",
        "\n",
        "torch.save(model.state_dict(), f\"last_checkpoint_epoch_{num_epochs}.pth\")\n",
        "\n",
        "plt.plot([x[0] for x in history], 'b', label='train')\n",
        "plt.plot([x[1] for x in history], 'r--',label='validation')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# test_loss = test()\n",
        "# print(f\"Test loss: {test_loss: .8f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MybMSF7KMV1K"
      },
      "outputs": [],
      "source": [
        "!cp /content/best_checkpoint_epoch.pth /content/drive/MyDrive/ai_study/kmw_prediction.pth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnomKGtLGF79"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "answer = pd.read_csv(\"/content/drive/MyDrive/ai/answer_sample.csv\")\n",
        "folder = os.listdir(\"/content/drive/Shareddrives/미세먼지_예측_대회/TEST\")\n",
        "model2 = Seq2Seq(enc, dec, device, 0).to(device)\n",
        "model2.load_state_dict(torch.load(\"/content/drive/MyDrive/ai/kmw_prediction.pth\"))\n",
        "\n",
        "measurement = answer['측정소'].drop_duplicates()\n",
        "ans_idx = 0\n",
        "\n",
        "dataX=[]\n",
        "for name in measurement:\n",
        "    df = pd.read_csv(\"/content/drive/Shareddrives/미세먼지_예측_대회/TEST/\"+name+\".csv\").drop([\"일시\"],axis=1)\n",
        "    test_x = np.array(df)\n",
        "    for idx in range(0,df.shape[0],48):\n",
        "        sliced_x = test_x[idx:idx+48].transpose()\n",
        "        dataX.append(sliced_x)\n",
        "        \n",
        "final_data = np.array(dataX)\n",
        "x = torch.FloatTensor(final_data).cuda()\n",
        "y_pred = np.array(model2(x, x).unsqueeze(2).cpu().detach())\n",
        "\n",
        "for i in range(y_pred.shape[0]):\n",
        "  for j in range(y_pred.shape[1]):\n",
        "      answer['PM2.5'][i*72+j] = y_pred[i, j]\n",
        "answer.to_csv(\"/content/drive/MyDrive/answer_ANN.csv\",encoding='utf-8')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = []\n",
        "\n",
        "for i, batch in enumerate(val_dataloader):\n",
        "    x, y = batch\n",
        "    output = model(x,y)\n",
        "    out.append(output.data.cpu().numpy()[1,:])\n",
        "    break\n",
        "\n",
        "label_y = valY_tensor.cpu().numpy()[1,:]\n",
        "len_72 = []\n",
        "for i in range(72) :\n",
        "  len_72.append(i+1)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(len_72, label_y, label='Actual Data')\n",
        "plt.scatter(len_72, out, label='Predicted Data')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zUxC79SVlI-s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNa4xnG4siadqXZ/pdKlfqO",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}